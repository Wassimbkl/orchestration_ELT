Lab 9 â€“ Mini-projet dâ€™orchestration ELT
ğŸ¯ Objectifs

Construire un pipeline complet avec Airflow pour orchestrer un processus ELT sur GCP :

Extraction dâ€™un fichier CSV depuis une API.

Ingestion dans Cloud Storage.

Transformation avec Pandas.

Chargement dans BigQuery.

ğŸ”§ PrÃ©requis

Airflow fonctionnel (Lab 6).

Bucket GCP existant (gs://data-cloud-wassim).

Dataset BigQuery crÃ©Ã© (data).

Service account avec les rÃ´les nÃ©cessaires pour BigQuery et Cloud Storage.

ğŸ“ Ã‰tapes
1ï¸âƒ£ Extraction depuis une API

CrÃ©er un script Python pour tÃ©lÃ©charger le CSV :

# /tmp/extract_covid.py
import requests

url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv"
r = requests.get(url)
with open("/tmp/covid.csv", "wb") as f:
    f.write(r.content)

print("Fichier CSV tÃ©lÃ©chargÃ© : /tmp/covid.csv")


ExÃ©cution :

python3 /tmp/extract_covid.py



2ï¸âƒ£ Ingestion dans Cloud Storage

Uploader le CSV vers le bucket GCP :

gsutil cp /tmp/covid.csv gs://data-cloud-wassim/raw/covid.csv




3ï¸âƒ£ Transformation avec Pandas

Nettoyer le CSV pour ne garder que location et new_cases :

# /tmp/transform_covid.py
import pandas as pd

df = pd.read_csv("/tmp/covid.csv")
df_clean = df[["location", "new_cases"]]
df_clean.to_csv("/tmp/covid_clean.csv", index=False)

print("CSV transformÃ© crÃ©Ã© : /tmp/covid_clean.csv")


ExÃ©cution :

python3 /tmp/transform_covid.py
cat /tmp/covid_clean.csv | head



4ï¸âƒ£ Chargement dans BigQuery

Charger le CSV transformÃ© dans BigQuery :

bq --location=EU load \
--autodetect \
--source_format=CSV \
data.covid_clean \
gs://data-cloud-wassim/raw/covid_clean.csv


Capture dâ€™Ã©cran : table covid_clean crÃ©Ã©e dans BigQuery.

5ï¸âƒ£ DAG Airflow

Assembler les tÃ¢ches dans un DAG Airflow /home/imadb/airflow/dags/covid_pipeline.py :

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import subprocess

default_args = {
    'start_date': datetime(2025, 9, 26),
}

def extract():
    import requests
    url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv"
    r = requests.get(url)
    with open("/tmp/covid.csv", "wb") as f:
        f.write(r.content)

def ingest():
    subprocess.run(["gsutil", "cp", "/tmp/covid.csv", "gs://data-cloud-wassim/raw/covid.csv"], check=True)

def transform():
    import pandas as pd
    df = pd.read_csv("/tmp/covid.csv")
    df2 = df[["location", "new_cases"]]
    df2.to_csv("/tmp/covid_clean.csv", index=False)

def load():
    subprocess.run([
        "bq", "load", "--autodetect", "--source_format=CSV",
        "data.covid_clean", "gs://data-cloud-wassim/raw/covid_clean.csv"
    ], check=True)

with DAG("covid_pipeline", schedule_interval="@daily", default_args=default_args, catchup=False) as dag:
    t1 = PythonOperator(task_id="extract", python_callable=extract)
    t2 = PythonOperator(task_id="ingest", python_callable=ingest)
    t3 = PythonOperator(task_id="transform", python_callable=transform)
    t4 = PythonOperator(task_id="load", python_callable=load)

    t1 >> t2 >> t3 >> t4




6ï¸âƒ£ VÃ©rification dans BigQuery

ExÃ©cuter la requÃªte SQL pour vÃ©rifier les donnÃ©es :

SELECT location, SUM(new_cases) as total_cases
FROM `data-cloud-mydigitalschool.data.covid_clean`
GROUP BY location
ORDER BY total_cases DESC;




ğŸ“ Organisation des fichiers
/tmp/extract_covid.py
/tmp/transform_covid.py
/home/imadb/airflow/dags/covid_pipeline.py



Toutes les commandes gsutil et bq doivent Ãªtre exÃ©cutÃ©es avec un compte de service ayant les bons scopes (cloud-platform).

Le DAG Airflow orchestre lâ€™ETL complet : extraction â†’ ingestion â†’ transformation â†’ chargement.

Les captures dâ€™Ã©cran doivent montrer :

TÃ©lÃ©chargement CSV

Bucket GCS avec fichiers

CSV transformÃ©

Table BigQuery

DAG Airflow

RÃ©sultats de la requÃªte SQL
